{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG FAQ-app\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "loader = CSVLoader('dockuno.csv')\n",
    "dataset = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_18340\\4073483054.py:5: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  myembedmodels = OllamaEmbeddings(model=\"all-minilm\")\n"
     ]
    }
   ],
   "source": [
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# myembedmodels = OpenAIEmbeddings(openai_api_key=\"\")\n",
    "\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "myembedmodels = OllamaEmbeddings(model=\"all-minilm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: store embeddings into Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -qU langchain-community faiss-cpu\n",
    "from langchain.vectorstores import FAISS\n",
    "db = FAISS.from_documents( documents=dataset , embedding=myembedmodels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db.similarity_search(\"what is the fees in dollars?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Prepare template prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dockuno_template = \"\"\"\n",
    "You are a world class business development representative for Dockuno program at eduLearn.ai .\n",
    "I will share a prospect's message with you and you will give me the best answer that I should send to this prospect based on past best practices, and you will follow ALL of the rules below:\n",
    "1/ Response should be very similar or even identical to the past best practies, in terms of length, ton of voice, logical arguments and other details\n",
    "2/ If the best practice are irrelevant, then try to mimic the style of the best practice to prospect's message\n",
    "\n",
    "Below is a message I received from the prospect:\n",
    "{message}\n",
    "\n",
    "Here is a list of best practies of how we normally respond to prospect in similar scenarios:\n",
    "{best_practice}\n",
    "\n",
    "Please write the best response that I should send to this prospect:\n",
    "\"\"\"\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "myprompt = PromptTemplate(template=dockuno_template, input_variables=[\"message\", \"best_practice\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 : prepare llm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_18340\\1571441268.py:2: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  myllm = ollama.ChatOllama(model=\"tinyllama\", temperature=0)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ollama\n",
    "myllm = ollama.ChatOllama(model=\"tinyllama\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_18340\\4143750790.py:5: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = my_chain.run(message=\"what is the fees of dockuno in dollar\", best_practice = best_practice)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "my_chain = LLMChain(llm=myllm , prompt=myprompt)\n",
    "best_practice = db.similarity_search(\"what is the fees in dollars?\")\n",
    "response = my_chain.run(message=\"what is the fees of dockuno in dollar\", best_practice = best_practice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear [Prospect],\n",
      "\n",
      "Thank you for reaching out to us regarding your interest in our Dockuno program. We are excited to provide you with more information about our course structure and pricing.\n",
      "\n",
      "Our course is designed to cover a variety of Docker concepts, including virtualization, containers, networking, and orchestration. The modules are structured into video lectures, hands-on labs, quizzes, and final project assessments. We believe that this approach provides you with a comprehensive understanding of Docker technology, as well as practical experience in using it.\n",
      "\n",
      "Our course fee is [insert price], which includes all materials and support throughout the program. Our pricing is competitive compared to other similar courses on the market.\n",
      "\n",
      "We understand that time is valuable, so we offer self-paced learners the option to complete the course at their own convenience. We also provide live Q&A sessions periodically where you can ask questions directly to our instructors. These sessions help clarify doubts and provide additional insight into Docker technology.\n",
      "\n",
      "We believe that our Dockuno program is a great investment in your professional development, and we are confident that it will provide you with the knowledge and skills needed to succeed in your chosen field. If you have any questions or concerns, please do not hesitate to contact us.\n",
      "\n",
      "Thank you for considering our Dockuno program. We look forward to hearing from you soon!\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Name]\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
